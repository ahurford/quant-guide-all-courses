# Statistics in R {#statistics}

Statistics is an important part of using R, and in various situations when you are using data, you might find useful to know certain functions or certain ways to test your data. The following section is to show some useful functions you might need to test your data added to some previous information regarding statistics in general

## Hypothesis Testing
  Testing data begins by considering two hypothesis, which are called `null hypothesis` $H_0$ and `alternative hypothesis` $H_1$. And they contain statements about the population/data under study
  Suppose we are interested in testing if the mean of a population `x` compares to the mean of a population `y`. Our `null hypothesis` and `alternative hypothesis` would look like this:
  
  $H_0: \mu_x = \mu_y$
  
  $H_1: \mu_x \neq \mu_y$
  
## Types of testing
  Depending on what you want to test and what you want to prove there are differente names for different types of testing. One of them being :
  Two-sided tests:  
  $H_0: \mu_x = \mu_y$
  
  $H_1: \mu_x \neq \mu_y$
  
  And One-sided tests:
    Lower-tailed test/less:
    
  $H_0: \mu_x \ge \mu_y$
  
  $H_1: \mu_x < \mu_y$
    
  Upper-tailed tests/greater:
    
  $H_0: \mu_x \leq \mu_y$
  
  $H_1: \mu_x > \mu_y$
  
## Significance level
  In statistics tests will give you a p-value that you will want to compare to a significance. This significance level will determine the probability of us rejecting our `null hypothesis` when is actually true. The level of significance is denoted by $\alpha$(alpha).And in most settings a significance level of `0.05` or `0.01` is used, which means there is only `5%` or `1%` probability of rejecting the null hypothesis when it is in fact true.

## P-value
  The p-value is a measure of probability that an observed difference could have occurred just by random chance. The smaller the p-value is, the greater the statistical significance of the observed difference.

## t.test
Sometimes within data, we want to compare our sample mean to our population. A t-test will help us compare them and determine if the two are statistically different.

Here is an example of how to compute a t.test within R.


```{r}
x <- rnorm(10)
x
```

With an $\alpha = 0.1$ we want to test that the mean of `x` is equal to `0`. Our two hypothesis are:

$H_0: \mu = 0$

$H_1: \mu \neq 0$

```{r}
t.test(x)
```

Running this code gives us a way analysis the data by comparing the `p-value` to our significance level $\alpha = 0.1$. Since we're getting a `p-value > 0.1`, we do not have enough evidence to reject the `null hypothesis`.

The `t.test` function can be expanded by filling arguments or changing arguments as required. For example, we can test two means to see if one is greater than, less than, or perform a two-sided t.test.
```
t.test(x, y = NULL,
       alternative = c("two.sided", "less", "greater"),
       mu = 0, paired = FALSE, var.equal = FALSE,
       conf.level = 0.95, ...)
       
       x(required) : data to be analized
       y(optional) : data to be compared to. The default value is NULL
       
       alternative (optional) : this refers to what you want to test: ("two.sided","less","greater").
          By default this value will be "two.sided"
       mu (optional) : indicates the true value of the mean. Default value = 0
       
       paired (optional) :indicates whether you want a paired t-test. Default value = FALSE
       
       var.equal (optional) : indicates if the variances are equal. Default value = FALSE
       
       conf.level (optional) : confidence level of interval. Default value = 0.95
       
```  

## Correlation
Correlation $r$  is a statistical measure that describes the extent to which two variables are linearly related (meaning they change together at a constant rate). While $r^2$ is the correlation coefficient and measures the proportion of the variance of one variable that can be explained by the straight-line dependence on the other variable. 

Association between variables describes the connection between changes in one variable and changes in the other. Positively associated variables refer to when an increase in the other accompanies an increase in one variable. A negative association refers to when a decrease in the other accompanies an increase in one variable.

The plots below show the visual representation of different $r$ values:

```{r, results=F, include=F}

data <- read.csv("correlation_data.csv")
reg1 <- lm(y ~ x, data=data)
reg2 <- lm(y.1 ~ x, data = data)
reg3 <- lm(y.3 ~ x, data = data)

```
```{r}
plot(data$x, data$y, xlab="x", ylab="y",
     main="Plot 1: r = 1", pch=19, col="green")
abline(reg1, col="green")

plot(data$x, data$y.1 ,xlab="x", ylab="y",
     main="Plot 2: r = -1", pch=19, col="red")
abline(reg2, col="red")

plot(data$x, data$y.3, xlab="x", ylab="y",
     main="Plot 3: r = 0.17", pch=19)
abline(reg3, col="blue")

```

- A $r=1$ means there is a perfect positive correlation between two variables (Plot 1).
- A $r=-1$ means there is a perfect negative correlation between two variables (Plot 2).
- A $r=0$ means there is no correlation between the two variables. 


